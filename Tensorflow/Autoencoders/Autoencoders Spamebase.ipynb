{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = input_data.read_data_sets('input/data')#, source_url='http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/')\n",
    "data = pd.read_csv('spambase.data.txt', header=None)\n",
    "data.rename(columns={57:'is_spam'}, inplace=True)\n",
    "X_train = []\n",
    "Y_train = []\n",
    "with open(\"spambase.data.txt\") as f:\n",
    "    for i,line in enumerate(f):\n",
    "        line= line.strip()\n",
    "        line = line.split(',')\n",
    "        line2 = [float(i) for i in line]\n",
    "        X_train.append(line2[:len(line2)-2])\n",
    "        Y_train.append(line2[len(line2)-1:][0])\n",
    "session = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.000001\n",
    "num_steps = 4100\n",
    "batch_size = 1\n",
    "\n",
    "display_step = 100\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "num_hidden_1 = 20 # 1st layer num features\n",
    "num_input = 56 # MNIST data input (img shape: 28*28)\n",
    "\n",
    "# tf Graph input (only pictures)\n",
    "X = tf.placeholder(\"float32\", [None, num_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1])),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_1, num_input])),\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([num_input])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
    "                                   biases['encoder_b1']))\n",
    "    return layer_1\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    # Decoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),\n",
    "                                   biases['decoder_b1']))\n",
    "    return layer_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "loss = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Minibatch Loss: 66.429367\n",
      "Step 1: Minibatch Loss: 182.261795\n",
      "Step 100: Minibatch Loss: 1.090351\n",
      "Step 200: Minibatch Loss: 350.097168\n",
      "Step 300: Minibatch Loss: 64.002380\n",
      "Step 400: Minibatch Loss: 0.580922\n",
      "Step 500: Minibatch Loss: 4.756402\n",
      "Step 600: Minibatch Loss: 664.643555\n",
      "Step 700: Minibatch Loss: 55.914375\n",
      "Step 800: Minibatch Loss: 3564.736572\n",
      "Step 900: Minibatch Loss: 11.255156\n",
      "Step 1000: Minibatch Loss: 9016.593750\n",
      "Step 1100: Minibatch Loss: 26.295574\n",
      "Step 1200: Minibatch Loss: 369.945526\n",
      "Step 1300: Minibatch Loss: 6.237895\n",
      "Step 1400: Minibatch Loss: 3.353933\n",
      "Step 1500: Minibatch Loss: 85.486534\n",
      "Step 1600: Minibatch Loss: 288.014709\n",
      "Step 1700: Minibatch Loss: 0.321673\n",
      "Step 1800: Minibatch Loss: 2.745939\n",
      "Step 1900: Minibatch Loss: 1.379956\n",
      "Step 2000: Minibatch Loss: 2.981306\n",
      "Step 2100: Minibatch Loss: 4.764340\n",
      "Step 2200: Minibatch Loss: 2.574328\n",
      "Step 2300: Minibatch Loss: 0.380571\n",
      "Step 2400: Minibatch Loss: 2.670952\n",
      "Step 2500: Minibatch Loss: 0.720108\n",
      "Step 2600: Minibatch Loss: 0.750966\n",
      "Step 2700: Minibatch Loss: 1.251683\n",
      "Step 2800: Minibatch Loss: 0.893675\n",
      "Step 2900: Minibatch Loss: 3.021456\n",
      "Step 3000: Minibatch Loss: 0.745230\n",
      "Step 3100: Minibatch Loss: 3.748216\n",
      "Step 3200: Minibatch Loss: 105.401123\n",
      "Step 3300: Minibatch Loss: 4.057201\n",
      "Step 3400: Minibatch Loss: 711.536438\n",
      "Step 3500: Minibatch Loss: 2.330969\n",
      "Step 3600: Minibatch Loss: 0.320270\n",
      "Step 3700: Minibatch Loss: 0.418535\n",
      "Step 3800: Minibatch Loss: 7.154488\n",
      "Step 3900: Minibatch Loss: 1.449544\n",
      "Step 4000: Minibatch Loss: 3.888415\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "# Training\n",
    "for i in range(1):\n",
    "    for index in range(num_steps):\n",
    "        # Prepare Data\n",
    "    # Get the next batch of MNIST data (only images are needed, not labels)\n",
    "        batch_x = X_train[index * batch_size: (index + 1) * batch_size]\n",
    "    # Run optimization op (backprop) and cost op (to get loss value)\n",
    "        _, l = sess.run([optimizer, loss], feed_dict={X: batch_x})\n",
    "    # Display logs per step\n",
    "        if index % display_step == 0 or index == 1:\n",
    "            print('Step %i: Minibatch Loss: %f' % (index, l))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_data = []\n",
    "g = sess.run(decoder_op, feed_dict={X:X_train})\n",
    "reconstructed_data = list(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 56)"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(reconstructed_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of predictions:  74.20965058236273 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "log_reg = linear_model.LogisticRegression()\n",
    "log_reg.fit(reconstructed_data[:4000], Y_train[:4000])\n",
    "predicted_labels = log_reg.predict(reconstructed_data[4000:])\n",
    "score = accuracy_score(Y_train[4000:],predicted_labels)\n",
    "print(\"Accuracy of predictions: \" ,score * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_data_reduced_dim = []\n",
    "g = sess.run(encoder_op, feed_dict={X:X_train})\n",
    "reconstructed_data_reduced_dim = list(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 20)"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(reconstructed_data_reduced_dim).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of predictions:  72.04658901830283 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "log_reg = linear_model.LogisticRegression()\n",
    "log_reg.fit(reconstructed_data_reduced_dim[:4000], Y_train[:4000])\n",
    "predicted_labels = log_reg.predict(reconstructed_data_reduced_dim[4000:])\n",
    "score = accuracy_score(Y_train[4000:],predicted_labels)\n",
    "print(\"Accuracy of predictions: \" ,score * 100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
